# Trackmania RL Clone ğŸï¸

A **Trackmania-inspired top-down time attack** game made with **Pygame**, featuring a **Reinforcement Learning agent (PPO)** trained with **Stable-Baselines3**.

## âœ¨ Features

- âœ… Runs on macOS (Apple Silicon optimized with MPS)
- âœ… Human-playable with smooth controls
- âœ… RL training with PPO algorithm
- âœ… Best-time ghost replay system
- âœ… Web dashboard for monitoring training
- âœ… Hyperparameter optimization with Optuna

## ğŸš€ Quick Start

### 1. Setup

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2. Using the Management Script

The project includes a unified management script for all operations:

```bash
# View all available commands
python3 manage.py --help

# Play the game manually
python3 manage.py play

# Train the AI agent
python3 manage.py train

# Watch the trained AI play
python3 manage.py watch

# Launch the web dashboard
python3 manage.py dashboard

# View project information
python3 manage.py info
```

## ğŸ® Manual Play

```bash
python3 manage.py play
# or
python3 trackmania_clone.py
```

**Controls:**
- **Arrow Keys**: Steer and accelerate
- **R**: Reset
- **Space**: Pause
- **G**: Toggle best ghost
- **Esc**: Quit

**Goal:** Hit all checkpoints and finish the lap to beat your best time!

## ğŸ¤– AI Training

### Basic Training

```bash
python3 manage.py train
# or
python3 rl_train.py
```

### Advanced Training Options

```bash
# Visual training mode (slower, single core)
python3 manage.py train --visual

# Custom number of steps
python3 manage.py train --steps 1000000

# Use optimized hyperparameters
python3 manage.py train --use-best-params

# Continue training from checkpoint
python3 manage.py train --load logs/checkpoint.zip
```

**Outputs:**
- `ppo_timeattack.zip` - Trained model
- `vecnormalize.pkl` - Normalization statistics
- `logs/` - Training logs and checkpoints

## ğŸ‘€ Watch AI Play

```bash
python3 manage.py watch
# or
python3 rl_play.py
```

**Controls while watching:**
- **Space**: Pause/Resume
- **R**: Reset episode
- **Esc**: Quit

## ğŸ“Š Web Dashboard

Launch the interactive web dashboard to monitor training:

```bash
python3 manage.py dashboard
```

The dashboard provides:
- Real-time training metrics
- Learning curves and analytics
- Process management
- System statistics

## ğŸ”§ Hyperparameter Optimization

Optimize training hyperparameters using Optuna:

```bash
python3 manage.py optimize --trials 50
# or
python3 rl_optimize.py --trials 50
```

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ LICENSE                # License information
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ manage.py             # Unified management script
â”œâ”€â”€ trackmania_clone.py   # Manual play script
â”œâ”€â”€ rl_train.py           # Training script
â”œâ”€â”€ rl_play.py            # AI play script
â”œâ”€â”€ rl_optimize.py        # Hyperparameter optimization
â”œâ”€â”€ watch_pb_replay.py    # Replay viewer
â”œâ”€â”€ rewards_config.json   # Reward configuration
â”œâ”€â”€ pytest.ini            # Test configuration
â”œâ”€â”€ src/                  # Source code
â”‚   â”œâ”€â”€ core/            # Core game logic
â”‚   â”œâ”€â”€ game/            # Game components
â”‚   â”œâ”€â”€ rl/              # RL wrappers and utilities
â”‚   â””â”€â”€ ...
â”œâ”€â”€ dashboard/            # Web dashboard
â”œâ”€â”€ docs/                 # Documentation
â”‚   â”œâ”€â”€ HYPERPARAMETERS_GUIDE.md
â”‚   â”œâ”€â”€ OPTIMIZATION_GUIDE.md
â”‚   â”œâ”€â”€ REPLAY_GUIDE.md
â”‚   â”œâ”€â”€ GAMEPLAY_MECHANICS.md
â”‚   â”œâ”€â”€ ANTI_REGRESSION_GUIDE.md
â”‚   â””â”€â”€ HYPERPARAMS_EDITOR.md
â”œâ”€â”€ scripts/              # Utility scripts
â”œâ”€â”€ tests/                # Test suite
â”œâ”€â”€ data/                 # Generated data (gitignored)
â”‚   â”œâ”€â”€ models/          # Trained models
â”‚   â”‚   â”œâ”€â”€ production/  # Production-ready models
â”‚   â”‚   â””â”€â”€ archive/     # Archived models
â”‚   â”œâ”€â”€ checkpoints/     # Training checkpoints
â”‚   â”œâ”€â”€ monitoring/      # Training metrics (CSV)
â”‚   â”œâ”€â”€ logs/            # Application logs
â”‚   â”‚   â”œâ”€â”€ training/    # Training logs
â”‚   â”‚   â”œâ”€â”€ game/        # Game logs
â”‚   â”‚   â””â”€â”€ ai/          # AI play logs
â”‚   â””â”€â”€ optimization/    # Hyperparameter optimization results
â””â”€â”€ replays/              # Saved replays
```

## ğŸ“š Documentation

For detailed information, see the documentation in the `docs/` folder:

- **[Hyperparameters Guide](docs/HYPERPARAMETERS_GUIDE.md)** - Understanding and tuning hyperparameters
- **[Optimization Guide](docs/OPTIMIZATION_GUIDE.md)** - Hyperparameter optimization strategies
- **[Replay Guide](docs/REPLAY_GUIDE.md)** - Using the replay system
- **[Gameplay Mechanics](docs/GAMEPLAY_MECHANICS.md)** - Game mechanics and physics
- **[Anti-Regression Guide](docs/ANTI_REGRESSION_GUIDE.md)** - Preventing training regression
- **[Hyperparams Editor](docs/HYPERPARAMS_EDITOR.md)** - Editing hyperparameters

## ğŸ§ª Testing

Run the test suite:

```bash
pytest
# or
python3 -m pytest -v
```

## ğŸ§¹ Maintenance

Clean cache and temporary files:

```bash
python3 manage.py clean

# Also clean logs and checkpoints (careful!)
python3 manage.py clean --logs
```

## ğŸ¯ Reward Shaping

The RL agent uses **reward shaping** for faster learning:
- Distance to next checkpoint
- Speed bonus
- Off-track penalty
- Checkpoint completion rewards

Configuration can be modified in `rewards_config.json`.

## ğŸ’¡ Future Improvements

- [ ] Add raycast sensors for smoother wall avoidance
- [ ] Multiple tracks to reduce overfitting
- [ ] Procedurally generated tracks
- [ ] Multi-agent racing
- [ ] Advanced physics (tire grip, drift mechanics)

## ğŸ“ License

See [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

Contributions are welcome! Feel free to open issues or submit pull requests.

---

Made with â¤ï¸ using Python, Pygame, and Stable-Baselines3
