# ğŸ›ï¸ Guide des HyperparamÃ¨tres

## ParamÃ¨tres Actuels (Conservateurs & Stables)

```json
{
    "n_steps": 2048,          // âœ… Stable
    "batch_size": 256,        // âœ… Ã‰quilibrÃ©
    "gamma": 0.995,           // âœ… Long-terme
    "gae_lambda": 0.95,       // âœ… Standard
    "ent_coef": 0.005,        // âœ… Peu d'exploration (bon pour stabilitÃ©)
    "learning_rate": 0.0003,  // âœ… ModÃ©rÃ© (avec schedule)
    "clip_range": 0.2,        // âœ… Standard PPO
    "max_grad_norm": 0.5,     // âœ… Gradients contrÃ´lÃ©s
    "vf_coef": 0.5            // âœ… Value function standard
}
```

## ğŸ“Š Impact de Chaque ParamÃ¨tre

### 1. **n_steps** (Steps avant update)
- **Valeur** : `2048`
- **Impact** : Nombre de steps collectÃ©s avant de mettre Ã  jour le rÃ©seau
- â¬†ï¸ Plus haut = Plus stable, mais apprend plus lentement
- â¬‡ï¸ Plus bas = Apprend plus vite, mais peut Ãªtre instable
- **Recommandation** : `2048` pour stabilitÃ©, `1024` si tu veux plus de vitesse

### 2. **batch_size** (Taille des lots)
- **Valeur** : `256`
- **Impact** : Combien d'exemples utilisÃ©s par update
- â¬†ï¸ Plus haut = Gradients plus stables, mais moins de diversitÃ©
- â¬‡ï¸ Plus bas = Plus de diversitÃ©, mais gradients bruitÃ©s
- **Recommandation** : `256` est optimal pour la plupart des cas

### 3. **learning_rate** (Vitesse d'apprentissage)
- **Valeur** : `0.0003` (3e-4) avec **schedule dÃ©croissant**
- **Impact** : Taille des pas d'apprentissage
- â¬†ï¸ Plus haut = Apprend plus vite, mais risque d'oublier (catastrophic forgetting)
- â¬‡ï¸ Plus bas = Apprend lentement, mais stable
- **Recommandation** : 
  - DÃ©but : `5e-4` (rapide)
  - Milieu : `3e-4` (stable)
  - Fin : `1e-4` (fine-tuning)

### 4. **ent_coef** (Coefficient d'entropie / Exploration)
- **Valeur** : `0.005`
- **Impact** : Encourage l'IA Ã  explorer de nouvelles stratÃ©gies
- â¬†ï¸ Plus haut = Plus d'exploration (bon au dÃ©but)
- â¬‡ï¸ Plus bas = Exploitation (bon quand l'IA maÃ®trise)
- **Recommandation** : 
  - DÃ©but : `0.01` (explore)
  - AprÃ¨s 1M steps : `0.005` (exploite)

### 5. **gamma** (Discount factor)
- **Valeur** : `0.995`
- **Impact** : Importance des rÃ©compenses futures
- â¬†ï¸ Plus haut (proche de 1) = Pense trÃ¨s long-terme
- â¬‡ï¸ Plus bas = PrÃ©fÃ¨re rÃ©compenses immÃ©diates
- **Recommandation** : `0.995` pour circuits (long-terme)

### 6. **clip_range** (PPO Clip)
- **Valeur** : `0.2`
- **Impact** : Limite les changements brusques de politique
- â¬†ï¸ Plus haut = Permet plus de changements
- â¬‡ï¸ Plus bas = Plus conservateur
- **Recommandation** : `0.2` est standard PPO

## ğŸ¯ Quand Utiliser Quoi ?

### Apprentissage Rapide (mais risquÃ©)
```json
{
    "n_steps": 1024,
    "batch_size": 512,
    "learning_rate": 0.0005,
    "ent_coef": 0.01
}
```
âœ… Bon pour : ExpÃ©rimentation rapide
âŒ Risque : InstabilitÃ©, catastrophic forgetting

### Apprentissage Stable (recommandÃ©)
```json
{
    "n_steps": 2048,
    "batch_size": 256,
    "learning_rate": 0.0003,
    "ent_coef": 0.005
}
```
âœ… Bon pour : EntraÃ®nement long et fiable
âŒ InconvÃ©nient : Plus lent

### Fine-Tuning (aprÃ¨s 1M+ steps)
```json
{
    "n_steps": 2048,
    "batch_size": 128,
    "learning_rate": 0.0001,
    "ent_coef": 0.001
}
```
âœ… Bon pour : Optimiser un modÃ¨le dÃ©jÃ  bon
âŒ InconvÃ©nient : TrÃ¨s lent

## ğŸ”§ Comment Tester ?

1. **Lance avec paramÃ¨tres stables** (actuels)
2. **Observe pendant 500k steps**
3. Si Ã§a marche bien â†’ Continue
4. Si c'est trop lent â†’ Augmente `learning_rate` Ã  `5e-4`
5. Si c'est instable â†’ RÃ©duis `ent_coef` Ã  `0.003`

## ğŸ’¡ Astuce

Le **Learning Rate Schedule** (dÃ©jÃ  implÃ©mentÃ©) est plus important que les valeurs fixes !
Il commence rapide et ralentit automatiquement pour Ã©viter l'oubli.
