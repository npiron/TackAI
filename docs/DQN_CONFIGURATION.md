# üß† Configuration DQN - Guide Complet

## Vue d'Ensemble

Ce projet utilise **DQN (Deep Q-Network)** pour entra√Æner une IA √† conduire une voiture sur circuit avec des **contr√¥les discrets on/off** (pas de pourcentages d'acc√©l√©ration).

## üéÆ Espace d'Actions - Discrete(9)

L'espace d'actions est **compl√®tement discret** avec 9 actions possibles (boutons on/off uniquement):

| Action | Description | Steer | Accel | Brake |
|--------|-------------|-------|-------|-------|
| 0 | Idle (aucune entr√©e) | 0.0 | 0.0 | 0.0 |
| 1 | Accelerate | 0.0 | 1.0 | 0.0 |
| 2 | Brake | 0.0 | 0.0 | 1.0 |
| 3 | Left (tourner gauche) | -1.0 | 0.0 | 0.0 |
| 4 | Right (tourner droite) | 1.0 | 0.0 | 0.0 |
| 5 | Left + Accelerate | -1.0 | 1.0 | 0.0 |
| 6 | Right + Accelerate | 1.0 | 1.0 | 0.0 |
| 7 | Left + Brake | -1.0 | 0.0 | 1.0 |
| 8 | Right + Brake | 1.0 | 0.0 | 1.0 |

### ‚úÖ Avantages de 9 Actions

1. **Contr√¥le Complet**: Toutes les combinaisons n√©cessaires pour conduire
2. **Freinage dans les Virages**: Essentiel pour bien n√©gocier les courbes (actions 7 et 8)
3. **Contr√¥le Fin**: Tourner sans acc√©l√©rer pour ajustements pr√©cis (actions 3 et 4)
4. **100% Discret**: Aucun pourcentage, seulement on/off (0.0 ou 1.0)

### ‚ùå Ancien Syst√®me (5 Actions)

L'ancien syst√®me n'avait que 5 actions et manquait:
- ‚ùå Left + Brake
- ‚ùå Right + Brake
- ‚ùå Left seul
- ‚ùå Right seul

Ces actions sont **critiques** pour un contr√¥le optimal.

## ‚öôÔ∏è Hyperparam√®tres DQN

### Configuration Actuelle (Optimis√©e)

```python
{
    "buffer_size": 200_000,           # M√©moire de replay (200k transitions)
    "learning_starts": 5_000,         # Commence √† apprendre apr√®s 5k steps
    "batch_size": 256,                # Taille des lots d'apprentissage
    "gamma": 0.99,                    # Facteur de discount
    "train_freq": 4,                  # Entra√Æne tous les 4 steps
    "gradient_steps": 2,              # ‚ú® 2 mises √† jour par entra√Ænement (am√©lior√©)
    "target_update_interval": 1000,   # Mise √† jour du r√©seau cible
    "exploration_fraction": 0.15,     # ‚ú® Explore 15% du temps (am√©lior√©)
    "exploration_initial_eps": 1.0,   # 100% al√©atoire au d√©but
    "exploration_final_eps": 0.05,    # 5% al√©atoire √† la fin
    "learning_rate": "schedule"       # D√©cro√Æt progressivement
}
```

### üéØ Am√©liorations Apport√©es

1. **`gradient_steps: 2`** (avant: 1)
   - Plus de mises √† jour par step d'entra√Ænement
   - Apprentissage plus efficace
   
2. **`exploration_fraction: 0.15`** (avant: 0.1)
   - 15% du temps total consacr√© √† l'exploration
   - Meilleure d√©couverte de strat√©gies

3. **Action Space: 9** (avant: 5)
   - Contr√¥le complet de la voiture
   - Man≈ìuvres plus complexes possibles

### üìä Impact des Param√®tres Cl√©s

#### buffer_size (200,000)
- **R√¥le**: M√©moire des exp√©riences pass√©es
- ‚¨ÜÔ∏è Plus grand = Plus stable, mais plus de RAM
- ‚¨áÔ∏è Plus petit = Moins stable, mais plus rapide
- **200k** = Bon √©quilibre

#### gradient_steps (2)
- **R√¥le**: Nombre de mises √† jour du r√©seau par √©tape
- ‚¨ÜÔ∏è Plus = Apprend plus vite de chaque exp√©rience
- ‚¨áÔ∏è Moins = Plus conservateur
- **2** = Efficace sans surapprentissage

#### exploration_fraction (0.15)
- **R√¥le**: Portion du temps consacr√©e √† l'exploration al√©atoire
- ‚¨ÜÔ∏è Plus = Plus d'exploration (bon pour environnements complexes)
- ‚¨áÔ∏è Moins = Plus d'exploitation (bon si d√©j√† performant)
- **0.15** = 15% du temps total

#### gamma (0.99)
- **R√¥le**: Importance des r√©compenses futures
- **0.99** = Pense √† long terme (bon pour circuits)

## üèóÔ∏è Architecture R√©seau

```python
policy_kwargs = {
    "net_arch": [256, 256]  # 2 couches de 256 neurones
}
```

### Options d'Architecture

| Taille | Neurones | Usage |
|--------|----------|-------|
| Small | [64, 64] | Environnements simples, CPU |
| Medium | [256, 256] | **Recommand√©** - Bon √©quilibre |
| Large | [512, 512] | Environnements complexes, GPU fort |

**Actuel**: Medium ([256, 256]) - Optimal pour ce projet

## üìà Learning Rate Schedule

```python
def lr_schedule(progress_remaining):
    """
    D√©croissance progressive du learning rate:
    - D√©but: 1e-3 (exploration rapide)
    - Fin: 1e-5 (fine-tuning)
    """
    return 1e-5 + (1e-3 - 1e-5) * progress_remaining
```

**Avantage**: L'IA apprend vite au d√©but, puis se stabilise pour √©viter l'oubli catastrophique.

## üîÑ Comparaison DQN vs PPO

| Aspect | DQN | PPO |
|--------|-----|-----|
| Type | Off-Policy | On-Policy |
| M√©moire | Replay Buffer (200k) | Petit buffer |
| Efficacit√© | R√©utilise les donn√©es | Donn√©es jet√©es |
| Stabilit√© | Tr√®s stable | Stable |
| Vitesse | Plus lent | Plus rapide |
| **Meilleur pour** | Actions discr√®tes | Actions continues |

**Conclusion**: DQN est **parfait** pour ce projet car:
1. ‚úÖ Actions 100% discr√®tes (on/off)
2. ‚úÖ R√©utilise les exp√©riences (efficace)
3. ‚úÖ Tr√®s stable pour l'apprentissage

## üöÄ Utilisation

### Entra√Ænement Standard

```bash
python3 rl_train.py --steps 2000000
```

### Entra√Ænement Visuel (Debug)

```bash
python3 rl_train.py --visual --steps 100000
```

### Continuer un Entra√Ænement

```bash
python3 rl_train.py --load data/checkpoints/model.zip
```

### Avec Hyperparam√®tres Optimis√©s

```bash
python3 rl_train.py --use-best-params
```

## üìù Notes Importantes

### ‚úÖ Ce qui est Correct

1. **Actions Discr√®tes**: Discrete(9) - 100% on/off, pas de pourcentages
2. **DQN pour Discret**: DQN est optimal pour des actions discr√®tes
3. **Buffer de Replay**: 200k transitions = bonne m√©moire
4. **Learning Rate Schedule**: √âvite l'oubli catastrophique

### ‚ö†Ô∏è Points d'Attention

1. **RAM**: Buffer de 200k consomme ~2-3 GB de RAM
2. **Exploration**: Les 15% premiers du training sont al√©atoires
3. **Temps**: DQN prend plus de temps que PPO mais est plus stable

## üéì Pour Aller Plus Loin

- [Documentation Stable-Baselines3 DQN](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html)
- [Guide des Hyperparam√®tres](./HYPERPARAMETERS_GUIDE.md)
- [Guide d'Optimisation](./OPTIMIZATION_GUIDE.md)
